# -*- coding: utf-8 -*-
"""Test_ToTaki_4yr_flow4_optim_xin_ddm_torch_randomloader_both_ddm_splitcomm_baseline_good_e2e_gpu_early_good_onlyValid_wtrain_good_4yr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EspXddlbe-hEYexpssrcw5mLT7XMnNKx
"""

# -*- coding: utf-8 -*-
"""
Created on 08-16-2019

@author: Xin Huang
"""

from numpy import vstack
from numpy import argmax
from pandas import read_csv
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from torch import Tensor
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch.utils.data import random_split
from torch.nn import Linear
from torch.nn import ReLU
from torch.nn import Sigmoid
from torch.nn import Softmax
from torch.nn import Module
from torch.nn import Dropout
from torch.nn import BatchNorm1d
from torch.optim import SGD,RMSprop,Adam
from torch.nn import CrossEntropyLoss
from torch.nn import MSELoss
from torch.nn.init import kaiming_uniform_
from torch.nn.init import xavier_uniform_
from torch.utils.data import TensorDataset
import torch

_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# _device = torch.device('cpu')
print("torch.cuda.is_available:")
print(torch.cuda.is_available())
# print("torch.cuda.get_device_name(0):")
# print(torch.cuda.get_device_name(0))
# import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
import numpy as np

# from google.colab import drive
# drive.mount('content/')
# data = np.load('Sat_data_small.npz')
# data = np.load('/content/content/My Drive/Colab Notebooks/Sat_data_small.npz')
# data = np.load('/content/content/My Drive/Colab Notebooks/kddworkshop/train7.npz')
# data = np.load('/content/content/My Drive/Colab Notebooks/kddworkshop/fulldata/train10.npz')
# data = np.load('/content/content/My Drive/Colab Notebooks/kddworkshop/chenxi/testing_10_days.npz')
# data = np.load('/content/content/My Drive/Colab Notebooks/kddworkshop/fulldata/4yr_jan.npz')

NUM_LALBELS = 6
# prefix = '/content/content/My Drive/Colab Notebooks/kddworkshop/fulldata/'
# prefix = '/content/content/My Drive/Colab Notebooks/kddworkshop/weak/'
# prefix = '/content/content/My Drive/Colab Notebooks/kddworkshop/weak/'
prefix = '/umbc/rs/nasa-access/data/calipso-viirs-merged/aerosol_free/'

def equalRate(a, b):
  c = a-b
  d = np.where(c==0)
  print("equal labels:")
  print(d[0].shape[0])
  print(d[0].shape[0] * 1.0 / a.shape[0])

# calipso_data_list = ['CALIOP_N_Clay_1km','CALIOP_N_Clay_5km','CALIOP_Liq_Fraction_1km','CALIOP_Liq_Fraction_5km','CALIOP_Ice_Fraction_1km','CALIOP_Ice_Fraction_5km','CALIOP_Clay_Top_Altitude','CALIOP_Clay_Base_Altitude','CALIOP_Clay_Top_Temperature','CALIOP_Clay_Base_Temperature','CALIOP_Clay_Optical_Depth_532','CALIOP_Clay_Opacity_Flag','CALIOP_Clay_Integrated_Attenuated_Backscatter_532','CALIOP_Clay_Integrated_Attenuated_Backscatter_1064','CALIOP_Clay_Final_Lidar_Ratio_532','CALIOP_Clay_Color_Ratio','CALIOP_Alay_Aerosol_Type_Mode','CALIOP_Alay_Top_Altitude','CALIOP_Alay_Base_Altitude','CALIOP_Alay_Top_Temperature','CALIOP_Alay_Base_Temperature','CALIOP_Alay_Integrated_Attenuated_Backscatter_532','CALIOP_Alay_Integrated_Attenuated_Backscatter_1064','CALIOP_Alay_Color_Ratio','CALIOP_Alay_Optical_Depth_532']
# position_list = ['Latitude','Longitude']
# aux_data_list =['Surface_Temperature','Surface_Emissivity','IGBP_SurfaceType','SnowIceIndex']
# VIIRS_data_list=['VIIRS_SZA','VIIRS_SAA','VIIRS_VZA','VIIRS_VAA','VIIRS_M01','VIIRS_M02','VIIRS_M03','VIIRS_M04','VIIRS_M05','VIIRS_M06','VIIRS_M07','VIIRS_M08','VIIRS_M09','VIIRS_M10','VIIRS_M11','VIIRS_M12','VIIRS_M13','VIIRS_M14','VIIRS_M15','VIIRS_M16']
# label_list = ['Pixel_Label_Calipso', 'Pixel_Label_VIIRS']

def loadData(prefix, filename): 
  data = np.load(prefix + filename)
  # passive = 1
  #load common data
  latlon = data['latlon']
  iff = data['iff']

  X_v = data['viirs']
  Y_v = data['label']
  print ('X_v shape:')
  print (X_v.shape)
  print ('Y_v.shape:')
  print(Y_v)
  Y_v = np.delete(Y_v, 0, 1)
  print ('Y_v.shape:')


  X_c = data['calipso']
  Y_c = data['label']
  print ('X_c shape:')
  print (X_c.shape)
  print ('Y_c.shape:')
  Y_c = np.delete(Y_c, 1, 1)
  print ('Y_c.shape:')

  equalRate(data['label'][:,0], data['label'][:,1])

  inds_v,vals_v = np.where(Y_v>0)
  Y_v = Y_v[inds_v]
  X_v = X_v[inds_v]
  print ('X_v')
  print (X_v)

  inds_c,vals_c = np.where(Y_v>0)
  Y_c = Y_c[inds_c]
  X_c = X_c[inds_c]
  print ('X_c')
  print (X_c)

  # process common data
  Latlon = latlon[inds_v]
  Iff = iff[inds_v]

  print('original X_v: ', X_v.shape)
  rows = np.where((X_v[:,0] >= 0) & (X_v[:,0] <= 83) & (X_v[:,15] > 100) & (X_v[:,15] < 400) & (X_v[:,16] > 100) & (X_v[:,16] < 400) & (X_v[:,17] > 100) & (X_v[:,17] < 400) & (X_v[:,18] > 100) & (X_v[:,18] < 400) & (X_v[:,19] > 100) & (X_v[:,19] < 400) & (X_v[:,10] > 0))
  print("rows:", rows)
  print("rows.shape:", len(rows))

  Latlon = Latlon[rows]
  Iff = Iff[rows]
  print("Iff:", Iff.shape)
  # Iff = Iff[:, 0]
  # print("Iff0:", Iff.shape)

  Y_v = Y_v[rows]
  X_v = X_v[rows]

  Y_c = Y_c[rows]
  X_c = X_c[rows]

  print('after SZA X_v: ', X_v.shape)
  print('after SZA X_c: ', X_c.shape)

  #concanate common data
  # X_v = np.concatenate((X_v, Latlon, Iff), axis=1)
  # X_c = np.concatenate((X_c, Latlon, Iff), axis=1)
  X_c = np.concatenate((X_c, Latlon), axis=1)
  print (X_v.shape)
  print (X_c.shape)

  X_v = np.nan_to_num(X_v)
  X_c = np.nan_to_num(X_c)
  return X_v, X_c, Y_v, Y_c

# load the training data
# train_file = 'train10.npz'
# prefix = '/content/content/My Drive/Colab Notebooks/kddworkshop/weak/'
# prefix = '/umbc/rs/nasa-access/data/calipso-viirs-merged/aerosol_free'
# train_file = '2013_mon1.npz'
# train_file = '2013.npz'
train_file_1 = '2013.npz'
train_file_2 = '2014.npz'
train_file_3 = '2015.npz'
train_file_4 = '2016.npz'
#train_file_1 = '2013_mon1.npz'
#train_file_2 = '2014_mon1.npz'
#train_file_3 = '2015_mon1.npz'
#train_file_4 = '2016_mon1.npz'

# train_file = '4yr_jan.npz'
# X_v, X_c, Y_v, Y_c = loadData(prefix, train_file)
X_v_1, X_c_1, Y_v_1, Y_c_1 = loadData(prefix, train_file_1)
X_v_2, X_c_2, Y_v_2, Y_c_2 = loadData(prefix, train_file_2)
X_v_3, X_c_3, Y_v_3, Y_c_3 = loadData(prefix, train_file_3)
X_v_4, X_c_4, Y_v_4, Y_c_4 = loadData(prefix, train_file_4)
X_v = np.concatenate((X_v_1, X_v_2, X_v_3, X_v_4), axis=0)
X_c = np.concatenate((X_c_1, X_c_2, X_c_3, X_c_4), axis=0)
Y_v = np.concatenate((Y_v_1, Y_v_2, Y_v_3, Y_v_4), axis=0)
Y_c = np.concatenate((Y_c_1, Y_c_2, Y_c_3, Y_c_4), axis=0)

# start of memory optimization
# X_v_1, X_c_1, Y_v_1, Y_c_1 = loadData(prefix, train_file_1)
# X_v = X_v_1
# X_c = X_c_1
# Y_v = Y_v_1
# Y_c = Y_c_1

# X_v_1, X_c_1, Y_v_1, Y_c_1 = loadData(prefix, train_file_2)
# X_v = np.concatenate((X_v, X_v_1), axis = 0)
# X_c = np.concatenate((X_c, X_c_1), axis = 0)
# Y_v = np.concatenate((Y_v, Y_v_1), axis=0)
# Y_c = np.concatenate((Y_c, Y_c_1), axis=0)

# X_v_1, X_c_1, Y_v_1, Y_c_1 = loadData(prefix, train_file_3)
# X_v = np.concatenate((X_v, X_v_1), axis = 0)
# X_c = np.concatenate((X_c, X_c_1), axis = 0)
# Y_v = np.concatenate((Y_v, Y_v_1), axis=0)
# Y_c = np.concatenate((Y_c, Y_c_1), axis=0)

# X_v_1, X_c_1, Y_v_1, Y_c_1 = loadData(prefix, train_file_4)
# X_v = np.concatenate((X_v, X_v_1), axis = 0)
# X_c = np.concatenate((X_c, X_c_1), axis = 0)
# Y_v = np.concatenate((Y_v, Y_v_1), axis=0)
# Y_c = np.concatenate((Y_c, Y_c_1), axis=0)
# end of memory optimization

del X_v_1, X_c_1, Y_v_1, Y_c_1
del X_v_2, X_c_2, Y_v_2, Y_c_2
del X_v_3, X_c_3, Y_v_3, Y_c_3
del X_v_4, X_c_4, Y_v_4, Y_c_4

Y = np.concatenate((Y_v, Y_c), axis=1)
print ("X_v.shape:", X_v.shape)
#print (Y_v.shape)
print ("X_c.shape:", X_c.shape)
#print (Y_c.shape)
print ("Y.shape:", Y.shape)

# combine data and split latter to define ground truth for MLR
# from sklearn.linear_model import LinearRegression
n1=20
n2=25
X=np.concatenate((X_v, X_c), axis=1)
# Y=Y_v
print (X.shape)
# print (Y_v)

del X_v, X_c
del Y_v, Y_c

x_train, x_valid, y_train, y_valid = train_test_split(X, Y,
                                                    test_size=0.3,
                                                    random_state=0,
                                                    stratify=Y)

# x_ids = list(range(len(X)))
# x_train_ids, x_valid_ids, y_train, y_valid = train_test_split(x_ids, Y, test_size = 0.3, random_state=0, stratify=Y)
# x_train = X[x_train_ids]
# x_valid = X[x_valid_ids]

# feature scaling
from sklearn.preprocessing import StandardScaler
from pickle import load
from pickle import dump

sc_X = StandardScaler()
x_train=sc_X.fit_transform(x_train)
x_valid=sc_X.transform(x_valid)
# x_test=sc_X.fit_transform(x_test)

# save the scaler
dump(sc_X, open('/home/xinh1/access/ieee/' + '/model/scaler.pkl', 'wb'))
print("saved scaler done")
