# -*- coding: utf-8 -*-
"""Ben_access_project_read_viirs_samples (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TFfw_4gdw1CZzP7QvGG8HF0sKGhGKEgi
"""

import argparse
import glob
import torch
import numpy as np
from sklearn.metrics import accuracy_score
from numpy import vstack
from numpy import argmax
from pickle import load
from data_utils import load_test_data
from data_utils import prepare_data
from model import Deep_coral
from train import evaluate_model_tgt
from modelVae import Deep_Dual_VAE

from torch.utils.data import TensorDataset
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

from os import path
import glob
import random
import pandas as pd


# path_to_data = '/Users/nizhao/xin/access/data/from_ben/'

# sample_list = glob.glob( '{}/*hdf5'.format(path_to_data) )

# #sample_list = [f for f in sample_list if '2020' in f]
# #sample_list = [f for f in sample_list if '2020' in f]

# sample_list

# file = random.choice(sample_list)

# #file = [f for f in sample_list if '2020_01_15' in f][0]

# file

# if path.exists(file):
#     with pd.HDFStore(file) as store:
#         print(store.keys())

def loadOffTrackData(file, sc_X):
    print("file:", file)
    with pd.HDFStore(file) as store:
        print(store.keys())

    with pd.HDFStore(file) as store:
        data = store['sample']

    print(data.head())
    print(data.columns)

    SZA = np.array(data['viirs_solar_zenith'])
    SAA = np.array(data['viirs_solar_azimuth'])
    VZA = np.array(data['viirs_sensor_zenith'])
    VAA = np.array(data['viirs_sensor_azimuth'])

    M1 = np.array(data['M01'])
    M2 = np.array(data['M02'])
    M3 = np.array(data['M03'])
    M4 = np.array(data['M04'])
    M5 = np.array(data['M05'])
    M6 = np.array(data['M06'])
    M7 = np.array(data['M07'])
    M8 = np.array(data['M08'])
    M9 = np.array(data['M09'])
    M10 = np.array(data['M10'])
    M11 = np.array(data['M11'])
    M12 = np.array(data['M12'])
    M13 = np.array(data['M13'])
    M14 = np.array(data['M14'])
    M15 = np.array(data['M15'])
    M16 = np.array(data['M16'])

    lat = np.array(data['latitude'])
    lon = np.array(data['longitude'])

    # IFF = np.array(Data['Surface_Temperature'])

    viirs_data = np.stack(
        (SZA, SAA, VZA, VAA, M1, M2, M3, M4, M5, M6, M7, M8, M9, M10, M11, M12, M13, M14, M15, M16, lat, lon), axis=-1)
    print(viirs_data.shape)
    print(viirs_data)
    print(viirs_data[0:10, 0:5])
    print(viirs_data[0:10, 13:15])
    print(viirs_data[0:10, 15:])

    fake_calipso = np.zeros((viirs_data.shape[0], 25))
    print("fake_calipso.shape:", fake_calipso)
    fake_data = np.concatenate((viirs_data[:, 0:20], fake_calipso, viirs_data[:, 20:22]), axis=1)
    fake_data_t = sc_X.transform(fake_data)
    print("fake_data_t:", fake_data_t.shape)
    print(fake_data_t)
    viirs_data_input = np.concatenate((fake_data_t[:, 0:20], fake_data_t[:, 45:47]), axis=1)
    print("viirs_data_input:", viirs_data_input.shape)
    print(viirs_data_input)
    return viirs_data_input, data


# off track prediction evaluation
def evaluate_model_predict(test_dl, model, device):
    model.eval()
    predictions, actuals = list(), list()
    # test_steps = len(test_dl)
    # iter_test = iter(test_dl)
    for i, (target_data) in enumerate(test_dl):
        # for i in range(test_steps):
        # evaluate the model on the test set
        target_data = target_data[0]
        print("target_data:")
        print(target_data)
        print("target_data:", target_data.shape)
        print(target_data)
        if torch.cuda.is_available():
            target_data = target_data.to(device)

        tgt_data = target_data
        with torch.no_grad():
            yhat = model.predict(tgt_data)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        # actual = targets.cpu().numpy()
        # convert to class labels
        print("yhat before argmax:")
        print(yhat[0:8])
        yhat = argmax(yhat, axis=1)
        print("yhat after argmax:")
        print(yhat[0:200])
        # reshape for stacking
        # actual = actual.reshape((len(actual), 1))
        yhat = yhat.reshape((len(yhat), 1))
        # store
        predictions.append(yhat)
        # actuals.append(actual)
    predictions = vstack(predictions)
    # calculate accuracy
    # acc = accuracy_score(actuals, predictions)
    return predictions


# predict_data = prepare_data_predict(viirs_data_input)
# pred_res = evaluate_model_predict(predict_data, model, _device)
# print("pred_res:", pred_res.shape)
# print(pred_res[0:200])

def prepare_data_predict(X_tgt, bSize):
    # load the train dataset
    print("X_tgt:")
    print(X_tgt)

    # x_src = torch.from_numpy(X_src)
    # y_src = torch.from_numpy(Y_src_1)
    x_tgt = torch.from_numpy(X_tgt)
    # y_tgt = torch.from_numpy(Y_tgt_1)

    datasets = TensorDataset(x_tgt.float())
    # prepare data loaders
    train_dl = DataLoader(datasets, batch_size=bSize, shuffle=False)
    return train_dl


def loadPredictData(out_dir, file_name):
    file = out_dir + file_name;
    print("file:", file)
    with pd.HDFStore(file) as store:
        print(store.keys())

    with pd.HDFStore(file) as store:
        data = store['sample']

    print(data.head())
    print(data.columns)
    print("value counts of data prediction:")
    print(data['prediction'].value_counts())
    ax = data['prediction'].hist()  # s is an instance of Series
    fig = ax.get_figure()
    fig.savefig(out_dir + '/distribution/' + file_name + '.pdf')


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument("--predicting_data_path")
    parser.add_argument("--model_saving_path")
    parser.add_argument("--export_data_path")
    args = parser.parse_args()

    NUM_LALBELS = 3
    BATCH_SIZE = 512

    _device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print("torch.cuda.is_available:", torch.cuda.is_available())

    # load the trained model
    # model = Deep_coral(num_classes=NUM_LALBELS)
    # model = torch.load(args.model_saving_path + '/model.pth')
    # sc_X = load(open(args.model_saving_path + '/scaler.pkl', 'rb'))

    model = Deep_Dual_VAE(num_classes=NUM_LALBELS)
    model.load_state_dict(torch.load(args.model_saving_path + '/model_classifier_best.pt', map_location=torch.device('cpu')))
    sc_X = load(open(args.model_saving_path + '/scaler.pkl', 'rb'))

    # evaluate on the testing data
    test_files = glob.glob(args.predicting_data_path + '/*hdf5')
    for test_file in test_files:
        viirs_data_input, data = loadOffTrackData(test_file, sc_X)
        predict_data = prepare_data_predict(viirs_data_input, BATCH_SIZE)
        pred_res = evaluate_model_predict(predict_data, model, _device)
        print("pred_res:", pred_res.shape)
        print(pred_res[0:20])

        data['prediction'] = pred_res
        # out_dir = '/home/xinh1/access/ieee/model/out_file/'
        out_dir = args.export_data_path
        file_name = test_file[test_file.rfind('/', 0) + 1:]
        data.to_hdf(out_dir + file_name, key='sample', mode='w')
        print("Data:")
        print(data[0:10])

        loadPredictData(out_dir, file_name)

        # X_s_test, Y_s_test, X_t_test, Y_t_test = load_test_data(test_file, sc_X)
        # test_dat = prepare_data(X_s_test, Y_s_test, X_t_test, Y_t_test, BATCH_SIZE)
        # acc = evaluate_model_tgt(test_dat, model, _device)
        # print("Test on file:", test_file)
        # print('Accuracy is: %.3f' % acc)

#m1-m16 vae
# python predict_vae.py --predicting_data_path='/Users/nizhao/xin/access/newdata/from_ben/'  --model_saving_path='/Users/nizhao/xin/access/data/weak/saved_model/' --export_data_path='/Users/nizhao/xin/access/newdata/from_ben/to_ben/m16/vae/'


# m1-m16
# python predict.py --predicting_data_path='/Users/nizhao/xin/access/newdata/from_ben/'  --model_saving_path='/Users/nizhao/xin/access/data/weak/saved_model/' --export_data_path='/Users/nizhao/xin/access/newdata/from_ben/to_ben/m16/'

# 2022-07-11 train the cloud mask model
# 1. python train.py --training_data_path='/Users/nizhao/xin/access/data/cloudmask/train/'  --model_saving_path='/Users/nizhao/xin/access/data/cloudmask/saved_model/'
# 2. python predict.py --predicting_data_path='/Users/nizhao/xin/access/newdata/from_ben/'  --model_saving_path='/Users/nizhao/xin/access/data/cloudmask/saved_model/' --export_data_path='/Users/nizhao/xin/access/newdata/from_ben/to_ben/m16/cloudmask/'


# python predict.py --predicting_data_path='/Users/nizhao/xin/access/data/from_ben/'  --model_saving_path='./saved_model/' --export_data_path='/Users/nizhao/xin/access/data/from_ben/to_ben/'
# data['M01'].hist()

# python predict.py --predicting_data_path='/Users/nizhao/xin/access/newdata/from_ben/'  --model_saving_path='./saved_model/' --export_data_path='/Users/nizhao/xin/access/newdata/from_ben/to_ben/m16/'

# category 1: clear (no cloud no aerosol)
# category 2: liquid (liquid only no aerosol)
# category 3: ice (ice only no aerosol)


# for file in sample_list:

#     if path.exists(file):
#         with pd.HDFStore(file) as store:
#             data = store['sample']

#     print(data.shape)

#     print(data[ data[] ].shape )

