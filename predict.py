# -*- coding: utf-8 -*-
"""Ben_access_project_read_viirs_samples (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TFfw_4gdw1CZzP7QvGG8HF0sKGhGKEgi
"""

import argparse
import glob
import torch
import numpy as np
from sklearn.metrics import accuracy_score
from numpy import vstack
from numpy import argmax
from pickle import load
from data_utils import load_test_data
from data_utils import prepare_data
from model import Deep_coral
from train import evaluate_model_tgt

from torch.utils.data import TensorDataset
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

from os import path
import glob
import random
import pandas as pd

# path_to_data = '/Users/nizhao/xin/access/data/from_ben/'

# sample_list = glob.glob( '{}/*hdf5'.format(path_to_data) )

# #sample_list = [f for f in sample_list if '2020' in f]
# #sample_list = [f for f in sample_list if '2020' in f]

# sample_list

# file = random.choice(sample_list)

# #file = [f for f in sample_list if '2020_01_15' in f][0]

# file

# if path.exists(file):
#     with pd.HDFStore(file) as store:
#         print(store.keys())

def loadOffTrackData(file, sc_X): 
    print("file:", file)
    with pd.HDFStore(file) as store:
        print(store.keys())

    with pd.HDFStore(file) as store:
        data = store['random_sample']

    print(data.head())
    print(data.columns)

    SZA = np.array(data['viirs_solar_zenith']) 
    SAA = np.array(data['viirs_solar_azimuth']) 
    VZA = np.array(data['viirs_sensor_zenith']) 
    VAA = np.array(data['viirs_sensor_azimuth'])

    M1 = np.array(data['M01'])
    M2 = np.array(data['M02'])
    M3 = np.array(data['M03'])
    M4 = np.array(data['M04'])
    M5 = np.array(data['M05'])
    M6 = np.array(data['M06'])
    M7 = np.array(data['M07'])
    M8 = np.array(data['M08'])
    M9 = np.array(data['M09'])
    M10 = np.array(data['M10'])
    M11 = np.array(data['M11'])
    M12 = np.array(data['M12'])
    M13 = np.array(data['M13'])
    M14 = np.array(data['M14'])
    M15 = np.array(data['M15'])
    M16 = np.array(data['M16'])

    lat = np.array(data['latitude']) 
    lon = np.array(data['longitude']) 

    # IFF = np.array(Data['Surface_Temperature'])

    viirs_data = np.stack((SZA, SAA, VZA, VAA, M1, M2, M3, M4, M5, M6, M7, M8, M9, M10, M11, M12, M13, M14, M15, M16, lat, lon), axis=-1)
    print(viirs_data.shape)
    print(viirs_data)
    print(viirs_data[0:10, 0:5])
    print(viirs_data[0:10, 13:15])
    print(viirs_data[0:10, 15:])

    fake_calipso = np.zeros((viirs_data.shape[0], 25))
    print("fake_calipso.shape:", fake_calipso)
    fake_data = np.concatenate((viirs_data[:, 0:20], fake_calipso, viirs_data[:, 20:22]),  axis=1)
    fake_data_t = sc_X.transform(fake_data)
    print("fake_data_t:", fake_data_t.shape)
    print(fake_data_t)
    viirs_data_input = np.concatenate((fake_data_t[:, 0:20], fake_data_t[:, 45:47]),  axis=1)
    print("viirs_data_input:", viirs_data_input.shape)
    print(viirs_data_input)
    return viirs_data_input, data

# off track prediction evaluation  
def evaluate_model_predict(test_dl, model, device):
    model.eval()
    predictions, actuals = list(), list()
    # test_steps = len(test_dl)
    # iter_test = iter(test_dl)
    for i, (target_data) in enumerate(test_dl):
    # for i in range(test_steps):
        # evaluate the model on the test set
        target_data = target_data[0]
        print("target_data:")
        print(target_data)
        print("target_data:", target_data.shape)
        print(target_data)
        if torch.cuda.is_available():
          target_data = target_data.to(device)

        tgt_data = target_data
        with torch.no_grad():
          yhat = model.predict(tgt_data)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        # actual = targets.cpu().numpy()
        # convert to class labels
        print("yhat before argmax:")
        print(yhat[0:8])
        yhat = argmax(yhat, axis=1)
        print("yhat after argmax:")
        print(yhat[0:200])
        # reshape for stacking
        # actual = actual.reshape((len(actual), 1))
        yhat = yhat.reshape((len(yhat), 1))
        # store
        predictions.append(yhat)
        # actuals.append(actual)
    predictions = vstack(predictions)
    # calculate accuracy
    # acc = accuracy_score(actuals, predictions)
    return predictions

# predict_data = prepare_data_predict(viirs_data_input)
# pred_res = evaluate_model_predict(predict_data, model, _device)
# print("pred_res:", pred_res.shape)
# print(pred_res[0:200])

def prepare_data_predict(X_tgt, bSize):
    # load the train dataset
    print("X_tgt:")
    print(X_tgt)

    # x_src = torch.from_numpy(X_src)
    # y_src = torch.from_numpy(Y_src_1)
    x_tgt = torch.from_numpy(X_tgt)
    # y_tgt = torch.from_numpy(Y_tgt_1)

    datasets = TensorDataset(x_tgt.float())
    # prepare data loaders
    train_dl = DataLoader(datasets, batch_size=bSize, shuffle=True)
    return train_dl

def loadPredictData(file): 
    print("file:", file)
    with pd.HDFStore(file) as store:
        print(store.keys())

    with pd.HDFStore(file) as store:
        data = store['random_sample']

    print(data.head())
    print(data.columns)
    ax = data['prediction'].hist()  # s is an instance of Series
    fig = ax.get_figure()
    fig.savefig('./prediction.pdf')

if __name__ == "__main__":

  parser = argparse.ArgumentParser()
  parser.add_argument("--predicting_data_path")
  parser.add_argument("--model_saving_path")
  parser.add_argument("--export_data_path")
  args = parser.parse_args()

  NUM_LALBELS = 3
  BATCH_SIZE = 2048

  _device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  print("torch.cuda.is_available:", torch.cuda.is_available())

  # load the trained model
  model = Deep_coral(num_classes=NUM_LALBELS)
  model = torch.load(args.model_saving_path + '/model.pth')
  sc_X = load(open(args.model_saving_path + '/scaler.pkl', 'rb'))

  # evaluate on the testing data
  test_files = glob.glob(args.predicting_data_path + '/*hdf5')
  for test_file in test_files:
    viirs_data_input, data = loadOffTrackData(test_file, sc_X)
    predict_data = prepare_data_predict(viirs_data_input, BATCH_SIZE)
    pred_res = evaluate_model_predict(predict_data, model, _device)
    print("pred_res:", pred_res.shape)
    print(pred_res[0:20])

    data['prediction'] = pred_res
    #out_dir = '/home/xinh1/access/ieee/model/out_file/'
    out_dir = args.export_data_path
    file_name = test_file[test_file.rfind('/', 0)+1:]
    data.to_hdf(out_dir + file_name, key='random_sample', mode='w')
    print("Data:")
    print(data[0:10])

    loadPredictData(out_dir + file_name)


    # X_s_test, Y_s_test, X_t_test, Y_t_test = load_test_data(test_file, sc_X)
    # test_dat = prepare_data(X_s_test, Y_s_test, X_t_test, Y_t_test, BATCH_SIZE)
    # acc = evaluate_model_tgt(test_dat, model, _device)
    # print("Test on file:", test_file)
    # print('Accuracy is: %.3f' % acc)


# python predict.py --predicting_data_path='/Users/nizhao/xin/access/data/from_ben/'  --model_saving_path='./saved_model/' --export_data_path='/Users/nizhao/xin/access/data/from_ben/to_ben/'
#data['M01'].hist()

# python predict.py --predicting_data_path='/Users/nizhao/xin/access/newdata/from_ben/'  --model_saving_path='./saved_model/' --export_data_path='/Users/nizhao/xin/access/newdata/from_ben/to_ben/'

    #category 1: clear (no cloud no aerosol)
    #category 2: liquid (liquid only no aerosol)
    #category 3: ice (ice only no aerosol)



# for file in sample_list:
    
#     if path.exists(file):
#         with pd.HDFStore(file) as store:
#             data = store['sample']

#     print(data.shape)
    
#     print(data[ data[] ].shape )











