# -*- coding: utf-8 -*-
"""Ben_access_project_read_viirs_samples (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TFfw_4gdw1CZzP7QvGG8HF0sKGhGKEgi
"""

import argparse
import glob
import torch
import numpy as np
from sklearn.metrics import accuracy_score
from numpy import vstack
from numpy import argmax
from pickle import load
from data_utils import load_test_data
from data_utils import prepare_data
from model import Deep_coral
from train import evaluate_model_tgt

from torch.utils.data import TensorDataset
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

from os import path
import glob
import random
import pandas as pd

from netCDF4 import Dataset
import os, glob
import numpy as np
import h5py

# path_to_data = '/Users/nizhao/xin/access/data/from_ben/'

# sample_list = glob.glob( '{}/*hdf5'.format(path_to_data) )

# #sample_list = [f for f in sample_list if '2020' in f]
# #sample_list = [f for f in sample_list if '2020' in f]

# sample_list

# file = random.choice(sample_list)

# #file = [f for f in sample_list if '2020_01_15' in f][0]

# file

# if path.exists(file):
#     with pd.HDFStore(file) as store:
#         print(store.keys())

def loadSingleGranuleData(sc_X): 
    #here I just give the VIIRS location:
    # v02_path = '/content/content/My Drive/Colab Notebooks/Chenxi_Shared_Research/Data/VIIRS/'
    # v03_path = '/content/content/My Drive/Colab Notebooks//Chenxi_Shared_Research/Data/VIIRS/'
    v02_path = '/Users/nizhao/xin/access/data/chenxi/VIIRS/'
    v03_path = '/Users/nizhao/xin/access/data/chenxi/VIIRS/'
    # data = np.load(v03_path + 'VNP03MOD.A2014187.0200.001.2017255041028.nc')

    viirs_timeflag = '2014187.0200'
    v02_file = glob.glob(v02_path+'VNP02*'+viirs_timeflag+'*.nc')[0]
    v03_file = glob.glob(v03_path+'VNP03*'+viirs_timeflag+'*.nc')[0]

        #v03 = Dataset(v03_file,'r')
    # v03 = h5py.File(v03_file,'r')
    v03 = Dataset(v03_file,'r')
    v03_lon = v03['/geolocation_data/longitude'][:]
    v03_lat = v03['/geolocation_data/latitude'][:]
    v03_sza = v03['/geolocation_data/solar_zenith'][:]
    v03_vza = v03['/geolocation_data/sensor_zenith'][:]
    v03_saa = v03['/geolocation_data/solar_azimuth'][:]
    v03_vaa = v03['/geolocation_data/sensor_azimuth'][:]
    v03.close()

    v02 = Dataset(v02_file,'r')
    #v02 = h5py.File(v02_file,'r')
    #suppose you are using all of the 16 bands
    #create a placeholder for all of these datasets
    #you can also use 16 different variables if you want
    v02group = v02['/observation_data']
    v02_datasets = ['M01','M02','M03','M04','M05','M06','M07','M08',
                    'M09','M10','M11','M12','M13','M14','M15', 'M16']
    sw_bands = ['M01','M02','M03','M04','M05','M06','M07','M08','M09','M10','M11']
    ir_bands = ['M12','M13','M14','M15', 'M16']

    n_channel = len(v02_datasets) #16 channels

    v02_obs = np.full([v03_lon.shape[0],v03_lon.shape[1],n_channel],fill_value=np.nan)

    for i, v02_dataset in zip(np.arange(n_channel),v02_datasets):
        #sometimes some channels are not included, 
        #for example some sw bands are not included during nighttime
        #so here we need to check
        if (v02_dataset in v02group.variables.keys()):
            #for shortwave first
            if (v02_dataset in sw_bands):
                v02_data = v02['/observation_data/'+v02_dataset][:]
                #note that netcdf automatically apply data scale and offset to original data
                #v02_data_scale = v02['/observation_data/'+v02_dataset].scale_factor
                #v02_data_offset = v02['/observation_data/'+v02_dataset].add_offset
                v02_obs[:,:,i] = v02_data
            #then for longwave bands
            #for ir we need to convert radiance to brightness temperature
            if (v02_dataset in ir_bands):
                v02_data = v02['/observation_data/'+v02_dataset][:]
                v02_data_scale = v02['/observation_data/'+v02_dataset].scale_factor
                v02_data_offset = v02['/observation_data/'+v02_dataset].add_offset
                v02_data_lut = v02['/observation_data/'+v02_dataset+'_brightness_temperature_lut'][:]
                #note that netcdf automatically apply data scale and offset to original data
                #to convert it back to brightness temperature, we need to do the following steps
                temp = (v02_data - v02_data_offset) / v02_data_scale
                v02_obs[:,:,i] = v02_data_lut[np.int32(np.round(temp))]        
    v02.close()

    #here is an example of the 500th row, 500th col pixel
    print (v02_obs[499,499,:])
    #here is another example of the 500th row, 500th col pixel
    print (v02_obs[0,0,:])

    #for shortwave bands, you need to remove 65533 values,
    #for ir bands, you need to remove -999.9 values,

    v02_obs[v02_obs==65533] = np.nan
    v02_obs[v02_obs<0] = np.nan

    #here is an example of the 500th row, 500th col pixel
    print (v02_obs[499,499,:])
    #here is another example of the 500th row, 500th col pixel
    print (v02_obs[0,0,:])

    v02_obs = np.nan_to_num(v02_obs)


    viirs_data = np.stack((v03_sza, v03_saa, v03_vza, v03_vaa, v02_obs[:, :, 0], v02_obs[:, :, 1], v02_obs[:, :, 2], v02_obs[:, :, 3], v02_obs[:, :, 4], v02_obs[:, :, 5], v02_obs[:, :, 6], v02_obs[:, :, 7], v02_obs[:, :, 8], v02_obs[:, :, 9], v02_obs[:, :, 10], v02_obs[:, :, 11], v02_obs[:, :, 12], v02_obs[:, :, 13], v02_obs[:, :, 14], v02_obs[:, :, 15],v03_lat, v03_lon), axis=-1)
    viirs_data = np.reshape(viirs_data, ( -1, 22))

    print(viirs_data.shape)
    print(viirs_data)
    print(viirs_data[0:10, 0:5])
    print(viirs_data[0:10, 13:15])
    print(viirs_data[0:10, 15:])
    print(viirs_data[249000, :])

    fake_calipso = np.zeros((viirs_data.shape[0], 25))
    print("fake_calipso:", fake_calipso)
    fake_data = np.concatenate((viirs_data[:, 0:20], fake_calipso, viirs_data[:, 20:22]),  axis=1)
    fake_data_t = sc_X.transform(fake_data)
    print("fake_data_t:", fake_data_t.shape)
    print(fake_data_t)
    viirs_data_input = np.concatenate((fake_data_t[:, 0:20], fake_data_t[:, 45:47]),  axis=1)
    print("viirs_data_input:", viirs_data_input.shape)
    print(viirs_data_input)

    return viirs_data_input



def loadOffTrackData(file, sc_X): 
    print("file:", file)
    with pd.HDFStore(file) as store:
        print(store.keys())

    with pd.HDFStore(file) as store:
        data = store['sample']

    print(data.head())
    print(data.columns)

    SZA = np.array(data['viirs_solar_zenith']) 
    SAA = np.array(data['viirs_solar_azimuth']) 
    VZA = np.array(data['viirs_sensor_zenith']) 
    VAA = np.array(data['viirs_sensor_azimuth'])

    M1 = np.array(data['M01'])
    M2 = np.array(data['M02'])
    M3 = np.array(data['M03'])
    M4 = np.array(data['M04'])
    M5 = np.array(data['M05'])
    M6 = np.array(data['M06'])
    M7 = np.array(data['M07'])
    M8 = np.array(data['M08'])
    M9 = np.array(data['M09'])
    M10 = np.array(data['M10'])
    M11 = np.array(data['M11'])
    M12 = np.array(data['M12'])
    M13 = np.array(data['M13'])
    M14 = np.array(data['M14'])
    M15 = np.array(data['M15'])
    M16 = np.array(data['M16'])

    lat = np.array(data['latitude']) 
    lon = np.array(data['longitude']) 

    # IFF = np.array(Data['Surface_Temperature'])

    viirs_data = np.stack((SZA, SAA, VZA, VAA, M1, M2, M3, M4, M5, M6, M7, M8, M9, M10, M11, M12, M13, M14, M15, M16, lat, lon), axis=-1)
    print(viirs_data.shape)
    print(viirs_data)
    print(viirs_data[0:10, 0:5])
    print(viirs_data[0:10, 13:15])
    print(viirs_data[0:10, 15:])

    fake_calipso = np.zeros((viirs_data.shape[0], 25))
    print("fake_calipso.shape:", fake_calipso)
    fake_data = np.concatenate((viirs_data[:, 0:20], fake_calipso, viirs_data[:, 20:22]),  axis=1)
    fake_data_t = sc_X.transform(fake_data)
    print("fake_data_t:", fake_data_t.shape)
    print(fake_data_t)
    viirs_data_input = np.concatenate((fake_data_t[:, 0:20], fake_data_t[:, 45:47]),  axis=1)
    print("viirs_data_input:", viirs_data_input.shape)
    print(viirs_data_input)
    return viirs_data_input, data

# off track prediction evaluation  
def evaluate_model_predict(test_dl, model, device):
    model.eval()
    predictions, actuals = list(), list()
    # test_steps = len(test_dl)
    # iter_test = iter(test_dl)
    for i, (target_data) in enumerate(test_dl):
    # for i in range(test_steps):
        # evaluate the model on the test set
        target_data = target_data[0]
        print("target_data:")
        print(target_data)
        print("target_data:", target_data.shape)
        print(target_data)
        if torch.cuda.is_available():
          target_data = target_data.to(device)

        tgt_data = target_data
        with torch.no_grad():
          yhat = model.predict(tgt_data)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        # actual = targets.cpu().numpy()
        # convert to class labels
        print("yhat before argmax:")
        print(yhat[0:8])
        yhat = argmax(yhat, axis=1)
        print("yhat after argmax:")
        print(yhat[0:200])
        # reshape for stacking
        # actual = actual.reshape((len(actual), 1))
        yhat = yhat.reshape((len(yhat), 1))
        # store
        predictions.append(yhat)
        # actuals.append(actual)
    predictions = vstack(predictions)
    # calculate accuracy
    # acc = accuracy_score(actuals, predictions)
    return predictions

# predict_data = prepare_data_predict(viirs_data_input)
# pred_res = evaluate_model_predict(predict_data, model, _device)
# print("pred_res:", pred_res.shape)
# print(pred_res[0:200])

def prepare_data_predict(X_tgt, bSize):
    # load the train dataset
    print("X_tgt:")
    print(X_tgt)

    # x_src = torch.from_numpy(X_src)
    # y_src = torch.from_numpy(Y_src_1)
    x_tgt = torch.from_numpy(X_tgt)
    # y_tgt = torch.from_numpy(Y_tgt_1)

    datasets = TensorDataset(x_tgt.float())
    # prepare data loaders
    train_dl = DataLoader(datasets, batch_size=bSize, shuffle=False)
    return train_dl

def loadPredictData(out_dir, file_name): 
    file = out_dir + file_name;
    print("file:", file)
    with pd.HDFStore(file) as store:
        print(store.keys())

    with pd.HDFStore(file) as store:
        data = store['sample']

    print(data.head())
    print(data.columns)
    print("value counts of data prediction:")
    print(data['prediction'].value_counts())
    ax = data['prediction'].hist()  # s is an instance of Series
    fig = ax.get_figure()
    fig.savefig(out_dir + '/distribution/' + file_name + '.pdf')

if __name__ == "__main__":

  parser = argparse.ArgumentParser()
  parser.add_argument("--predicting_data_path")
  parser.add_argument("--model_saving_path")
  parser.add_argument("--export_data_path")
  args = parser.parse_args()

  NUM_LALBELS = 3
  BATCH_SIZE = 2048

  _device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  print("torch.cuda.is_available:", torch.cuda.is_available())


  # load the trained model
  model = Deep_coral(num_classes=NUM_LALBELS)
  model = torch.load(args.model_saving_path + '/model.pth')
  sc_X = load(open(args.model_saving_path + '/scaler.pkl', 'rb'))

  viirs_data_input = loadSingleGranuleData(sc_X)

  predict_data = prepare_data_predict(viirs_data_input, BATCH_SIZE)
  pred_res = evaluate_model_predict(predict_data, model, _device)
  print("pred_res:", pred_res.shape)
  print(pred_res[0:20])
  pred_res = np.reshape(pred_res, (3232, -1))


  v03_path = '/Users/nizhao/xin/access/data/chenxi/prediction/'

  viirs_timeflag = '2014187.0200'
  # v02_file = glob.glob(v02_path+'VNP02*'+viirs_timeflag+'*.nc')[0]
  v03_file = glob.glob(v03_path+'VNP03*'+viirs_timeflag+'*.nc')[0]

    #v03 = Dataset(v03_file,'r')
  # v03 = h5py.File(v03_file,'r')
  # v03 = Dataset(v03_file,'r')
  # v03_lon = v03['/geolocation_data/longitude'][:]
  # v03_lat = v03['/geolocation_data/latitude'][:]
  # v03_sza = v03['/geolocation_data/solar_zenith'][:]
  # v03_vza = v03['/geolocation_data/sensor_zenith'][:]
  # v03_saa = v03['/geolocation_data/solar_azimuth'][:]
  # v03_vaa = v03['/geolocation_data/sensor_azimuth'][:]

  # v03['geolocation_data/prediction'] = pred_res
  new_filename_1 = v03_path+'VNP03'+viirs_timeflag+'_prediction_onlyM16_1stmonth.nc'
  # v03.to_netcdf(path=new_filename_1)

  save_id = h5py.File( new_filename_1, 'w' )
  save_id.create_dataset( 'prediction', data=pred_res )
  save_id.close()

  print ('finished saving')

# 1 month train data 
  #1. python train.py --training_data_path='/Users/nizhao/xin/access/data/weak/train/'  --model_saving_path='/Users/nizhao/xin/access/data/weak/saved_model/'
  #2. python predict_chenxi_onlyM16.py --predicting_data_path='/Users/nizhao/xin/access/data/'  --model_saving_path='/Users/nizhao/xin/access/data/weak/saved_model/' --export_data_path='/Users/nizhao/xin/access/data/'

# 1. to train the model
# conda activate cloud-phase-prediction-env 
# 2. conda deactivate to predict_chenxin or predict ben
#python predict_chenxi_onlyM16.py --predicting_data_path='/Users/nizhao/xin/access/data/'  --model_saving_path='./saved_model/' --export_data_path='/Users/nizhao/xin/access/data/'


  # # read verify 
  # v03_path = '/Users/nizhao/xin/access/data/chenxi/VIIRS/'
  # viirs_timeflag = '2014187.0200'
  # v03_file = glob.glob(v03_path+'VNP03*'+viirs_timeflag+'*.nc')[0]

  # save_id = h5py.File( new_filename_1, 'r' )
  # res = save_id['prediction']
  # print(res.shape)
  # print(res)
  # save_id.close()

  # print ('finished reading')
  # v03.close()

  # # evaluate on the testing data
  # test_files = glob.glob(args.predicting_data_path + '/*hdf5')
  # for test_file in test_files:
  #   viirs_data_input, data = loadOffTrackData(test_file, sc_X)
  #   predict_data = prepare_data_predict(viirs_data_input, BATCH_SIZE)
  #   pred_res = evaluate_model_predict(predict_data, model, _device)
  #   print("pred_res:", pred_res.shape)
  #   print(pred_res[0:20])

  #   data['prediction'] = pred_res
  #   #out_dir = '/home/xinh1/access/ieee/model/out_file/'
  #   out_dir = args.export_data_path
  #   file_name = test_file[test_file.rfind('/', 0)+1:]
  #   data.to_hdf(out_dir + file_name, key='sample', mode='w')
  #   print("Data:")
  #   print(data[0:10])

    # loadPredictData(out_dir, file_name)






    # X_s_test, Y_s_test, X_t_test, Y_t_test = load_test_data(test_file, sc_X)
    # test_dat = prepare_data(X_s_test, Y_s_test, X_t_test, Y_t_test, BATCH_SIZE)
    # acc = evaluate_model_tgt(test_dat, model, _device)
    # print("Test on file:", test_file)
    # print('Accuracy is: %.3f' % acc)


# python predict.py --predicting_data_path='/Users/nizhao/xin/access/data/from_ben/'  --model_saving_path='./saved_model/' --export_data_path='/Users/nizhao/xin/access/data/from_ben/to_ben/'
#data['M01'].hist()

# python predict.py --predicting_data_path='/Users/nizhao/xin/access/newdata/from_ben/'  --model_saving_path='./saved_model/' --export_data_path='/Users/nizhao/xin/access/newdata/from_ben/to_ben/'

    #category 1: clear (no cloud no aerosol)
    #category 2: liquid (liquid only no aerosol)
    #category 3: ice (ice only no aerosol)



# for file in sample_list:
    
#     if path.exists(file):
#         with pd.HDFStore(file) as store:
#             data = store['sample']

#     print(data.shape)
    
#     print(data[ data[] ].shape )











